{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e98c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UET_2025\\DL\\BTL\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigcode/starcoder2-3b\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"lora_weight\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935a2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = r'''\n",
    "Given a list of integers `nums` of size `n`, return the sum of the two largest integers.\n",
    "\n",
    "Example:\n",
    "Input: `nums = [1, 2, 3, 4, 5]`\n",
    "Output: `9`\n",
    "\n",
    "Explanation:\n",
    "The two largest integers are 4 and 5, so the sum is 9.\n",
    "\n",
    "Constraints:\n",
    "- `n < 10000`\n",
    "\n",
    "\n",
    "Boilerplate code:\n",
    "```python\n",
    "def sum_of_two_largest(nums):\n",
    "    ...\n",
    "```\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437f4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = f\"\"\"### Problem:\n",
    "{prompt.strip()}\n",
    "\n",
    "Instruction: Write exactly one clean and efficient Python function that correctly solves the problem.\n",
    "The solution should be optimal in both time and space complexity. \n",
    "Do not include multiple solutions or explanations.\n",
    "\n",
    "### Solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Code ===\n",
      "def sum_of_two_largest(nums):\n",
      "    \"\"\"\n",
      "    :type nums: List[int]\n",
      "    :rtype: int\n",
      "    \"\"\"\n",
      "    largest = second_largest = -math.inf\n",
      "    for num in nums:\n",
      "        if num > largest:\n",
      "            second_largest = largest\n",
      "            largest = num\n",
      "        elif num > second_largest:\n",
      "            second_largest = num\n",
      "    return largest + second_largest\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Tokenize\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)\n",
    "\n",
    "# Cấu hình sinh text\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Sinh\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "# Giải mã\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Chỉ lấy phần sau \"### Solution:\"\n",
    "generated_code = full_output.split(\"### Solution:\")[-1].strip()\n",
    "generated_code = generated_code.split(\"\\n\\n<|end_of_solution|>\")[0].strip()\n",
    "\n",
    "print(\"=== Generated Code ===\")\n",
    "print(generated_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-asm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
