{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13750403,"sourceType":"datasetVersion","datasetId":8749438}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate peft bitsandbytes datasets trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:22:54.536794Z","iopub.execute_input":"2025-11-16T06:22:54.537027Z","iopub.status.idle":"2025-11-16T06:24:35.637503Z","shell.execute_reply.started":"2025-11-16T06:22:54.537004Z","shell.execute_reply":"2025-11-16T06:24:35.636695Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport random\nfrom pathlib import Path\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nimport bitsandbytes as bnb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:26:47.582249Z","iopub.execute_input":"2025-11-16T06:26:47.583031Z","iopub.status.idle":"2025-11-16T06:27:41.156555Z","shell.execute_reply.started":"2025-11-16T06:26:47.582990Z","shell.execute_reply":"2025-11-16T06:27:41.155540Z"}},"outputs":[{"name":"stderr","text":"2025-11-16 06:27:04.798230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763274425.288068      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763274425.421513      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# Đường dẫn dataset trên Kaggle (bạn có thể thay bằng path thực tế)\nPROBLEMS_DIR = \"/kaggle/input/pleasework/dataset/problems\"\nSOLUTIONS_DIR = \"/kaggle/input/pleasework/dataset/solution\"\n\ndef load_samples():\n    data = []\n    problem_files = sorted(Path(PROBLEMS_DIR).glob(\"Problem*.md\"))\n    for pf in problem_files:\n        qid = pf.stem.replace(\"Problem\", \"\")\n        sf = Path(SOLUTIONS_DIR) / f\"Solution{qid}.py\"\n        if not sf.exists():\n            continue\n        problem_text = pf.read_text().strip()\n        solution_code = sf.read_text().strip()\n\n        # Format prompt theo kiểu instruct\n        prompt = f\"\"\"\n            <|system|>\n            You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.\n            <|user|>\n            ### Problem\n            {problem_text}\n            \n            ### Write the solution in Python.\n            \n            <|assistant|>\n            ```python\n            {solution_code}\n            ```\n            \"\"\"\n\n        data.append({\"text\": prompt})\n    return data\n\nraw_data = load_samples()\nprint(f\"Loaded {len(raw_data)} samples.\")\ndataset = Dataset.from_list(raw_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:27:48.549840Z","iopub.execute_input":"2025-11-16T06:27:48.550538Z","iopub.status.idle":"2025-11-16T06:28:33.165310Z","shell.execute_reply.started":"2025-11-16T06:27:48.550515Z","shell.execute_reply":"2025-11-16T06:28:33.164419Z"}},"outputs":[{"name":"stdout","text":"Loaded 2869 samples.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load model in 4-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Chuẩn bị model cho QLoRA\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:28:43.302425Z","iopub.execute_input":"2025-11-16T06:28:43.303014Z","iopub.status.idle":"2025-11-16T06:28:56.117994Z","shell.execute_reply.started":"2025-11-16T06:28:43.302987Z","shell.execute_reply":"2025-11-16T06:28:56.117365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc5b60c3011456896c0abc1b8710b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc9b930cbf734f0a9b7d80a93e72a38d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5f80193ef040068b14c7961ed9691e"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4dd7e9e1412482cadd43a4a2c66eb24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f299c574a8384ae79c68b5fea6c724f7"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # DeepSeek-Coder dùng kiến trúc LLaMA-like\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:29:24.192029Z","iopub.execute_input":"2025-11-16T06:29:24.192787Z","iopub.status.idle":"2025-11-16T06:29:24.643965Z","shell.execute_reply.started":"2025-11-16T06:29:24.192758Z","shell.execute_reply":"2025-11-16T06:29:24.643202Z"}},"outputs":[{"name":"stdout","text":"trainable params: 25,165,824 || all params: 1,371,637,760 || trainable%: 1.8347\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=False,\n        max_length=1024\n    )\n\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset.column_names,\n    num_proc=2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:29:32.792673Z","iopub.execute_input":"2025-11-16T06:29:32.793252Z","iopub.status.idle":"2025-11-16T06:29:38.072586Z","shell.execute_reply.started":"2025-11-16T06:29:32.793227Z","shell.execute_reply":"2025-11-16T06:29:38.071615Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2869 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"042df68250634dd4a692bbd48a9c2b3d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./deepseek-coder-finetuned3b\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    save_strategy=\"steps\",\n    save_steps=100,\n    logging_steps=10,\n    num_train_epochs=3,\n    fp16=True,  \n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    report_to=\"none\",\n    greater_is_better=False,\n    save_total_limit=2,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    dataloader_num_workers=2,\n)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:30:02.055375Z","iopub.execute_input":"2025-11-16T06:30:02.056192Z","iopub.status.idle":"2025-11-16T06:30:02.089559Z","shell.execute_reply.started":"2025-11-16T06:30:02.056164Z","shell.execute_reply":"2025-11-16T06:30:02.088807Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset = tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:30:06.173355Z","iopub.execute_input":"2025-11-16T06:30:06.173937Z","iopub.status.idle":"2025-11-16T06:30:06.177555Z","shell.execute_reply.started":"2025-11-16T06:30:06.173910Z","shell.execute_reply":"2025-11-16T06:30:06.177009Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:30:08.811681Z","iopub.execute_input":"2025-11-16T06:30:08.812284Z","iopub.status.idle":"2025-11-16T08:31:04.756091Z","shell.execute_reply.started":"2025-11-16T06:30:08.812260Z","shell.execute_reply":"2025-11-16T08:31:04.755237Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/2944723658.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 32021}.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [540/540 2:00:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.888300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.691300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.531600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.419800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.411300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.399800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.415900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.371500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.365400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.406500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.386900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.344600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.346900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.357800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.365400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.375400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.326100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.363400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.314300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.306700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.355800</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.331600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.312900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.325700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.301700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.321900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.286400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.315600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.294500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.301600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.304100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.282700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.321500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.295800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.321200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.310800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.261700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.267100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.283400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.232600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.247300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.274500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.249700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.275500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.271800</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.243700</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.260900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.290000</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.269300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.290000</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.296500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.257600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.250200</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.300900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=540, training_loss=0.3369554016325209, metrics={'train_runtime': 7255.3516, 'train_samples_per_second': 1.186, 'train_steps_per_second': 0.074, 'total_flos': 6.786690676697088e+16, 'train_loss': 0.3369554016325209, 'epoch': 3.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:39:55.841483Z","iopub.execute_input":"2025-11-16T08:39:55.842254Z","iopub.status.idle":"2025-11-16T08:39:56.318519Z","shell.execute_reply.started":"2025-11-16T08:39:55.842220Z","shell.execute_reply":"2025-11-16T08:39:56.317632Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\nmodel_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\nadapter_path = \"/kaggle/working/deepseek-coder-finetuned3b\"  # thư mục bạn lưu bằng trainer.save_model()\n\n# Cấu hình 4-bit\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Gắn adapter QLoRA\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:43:32.156426Z","iopub.execute_input":"2025-11-16T08:43:32.157395Z","iopub.status.idle":"2025-11-16T08:43:36.481743Z","shell.execute_reply.started":"2025-11-16T08:43:32.157355Z","shell.execute_reply":"2025-11-16T08:43:36.481103Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(32256, 2048)\n        (layers): ModuleList(\n          (0-23): 24 x LlamaDecoderLayer(\n            (self_attn): LlamaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (up_proj): Linear4bit(in_features=2048, out_features=5504, bias=False)\n              (down_proj): Linear4bit(in_features=5504, out_features=2048, bias=False)\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=32256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"problem_text = r'''\nGiven two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.\nThe overall run time complexity should be O(log (m+n)).\n \nExample 1:\n\nInput: nums1 = [1,3], nums2 = [2]\nOutput: 2.00000\nExplanation: merged array = [1,2,3] and median is 2.\n\nExample 2:\n\nInput: nums1 = [1,2], nums2 = [3,4]\nOutput: 2.50000\nExplanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.\n\n \nConstraints:\n\nnums1.length == m\nnums2.length == n\n0 <= m <= 1000\n0 <= n <= 1000\n1 <= m + n <= 2000\n-106 <= nums1[i], nums2[i] <= 106\n\n\nBoilerplate code:\n```python\ndef findMedianSortedArrays(nums1, nums2):\n    ...\n```\n  '''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:47:59.769270Z","iopub.execute_input":"2025-11-16T08:47:59.770132Z","iopub.status.idle":"2025-11-16T08:47:59.773710Z","shell.execute_reply.started":"2025-11-16T08:47:59.770105Z","shell.execute_reply":"2025-11-16T08:47:59.773102Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def generate_solution(problem_text: str, max_new_tokens=300) -> str:\n    prompt = f\"\"\"\n            <|system|>\n            You are an expert Python programmer. You will be given a question (problem specification) and will generate a correct Python program that matches the specification and passes all tests.            \n            <|user|>\n            ### Problem\n            {problem_text}\n            \n            ### Write the solution in Python.\n            \n            <|assistant|>\n            ```python\n            \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens= 512,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    code = generated[len(prompt):]\n    if \"```\" in code:\n        code = code.split(\"```\")[0]\n    return code.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:48:26.998495Z","iopub.execute_input":"2025-11-16T08:48:26.999062Z","iopub.status.idle":"2025-11-16T08:48:27.004318Z","shell.execute_reply.started":"2025-11-16T08:48:26.999038Z","shell.execute_reply":"2025-11-16T08:48:27.003528Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"solution = generate_solution(problem_text)\nprint(solution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:48:29.093763Z","iopub.execute_input":"2025-11-16T08:48:29.094079Z","iopub.status.idle":"2025-11-16T08:49:08.540143Z","shell.execute_reply.started":"2025-11-16T08:48:29.094059Z","shell.execute_reply":"2025-11-16T08:49:08.539449Z"}},"outputs":[{"name":"stdout","text":"from typing import List\n\nclass Solution:\n    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -> float:\n        if len(nums1) > len(nums2):\n            nums1, nums2 = nums2, nums1\n        \n        x, y = len(nums1), len(nums2)\n        low, high = 0, x\n        \n        while low <= high:\n            partitionX = (low + high) // 2\n            partitionY = (x + y + 1) // 2 - partitionX\n            \n            maxLeftX = float('-inf') if partitionX == 0 else nums1[partitionX - 1]\n            minRightX = float('inf') if partitionX == x else nums1[partitionX]\n            \n            maxLeftY = float('-inf') if partitionY == 0 else nums2[partitionY - 1]\n            minRightY = float('inf') if partitionY == y else nums2[partitionY]\n            \n            if maxLeftX <= minRightY and maxLeftY <= minRightX:\n                if (x + y) % 2 == 0:\n                    return (max(maxLeftX, maxLeftY) + min(minRightX, minRightY)) / 2\n                else:\n                    return max(maxLeftX, maxLeftY)\n            elif maxLeftX > minRightY:\n                high = partitionX - 1\n            else:\n                low = partitionX + 1\n        \n        raise AssertionError(\"Unreachable code\")\n\ndef findMedianSortedArrays(nums1: List[int], nums2: List[int]) -> float:\n    return Solution().findMedianSortedArrays(nums1, nums2)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!zip -r /kaggle/working/deepseek-coder-finetuned.zip /kaggle/working/deepseek-coder-finetuned3b\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:40:54.208444Z","iopub.execute_input":"2025-11-16T08:40:54.209106Z","iopub.status.idle":"2025-11-16T08:41:32.855356Z","shell.execute_reply.started":"2025-11-16T08:40:54.209081Z","shell.execute_reply":"2025-11-16T08:41:32.854572Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/deepseek-coder-finetuned3b/ (stored 0%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/training_args.bin (deflated 51%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/ (stored 0%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/training_args.bin (deflated 51%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/optimizer.pt (deflated 9%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/tokenizer_config.json (deflated 88%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/README.md (deflated 66%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/adapter_config.json (deflated 56%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/chat_template.jinja (deflated 57%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/scheduler.pt (deflated 56%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/special_tokens_map.json (deflated 58%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/trainer_state.json (deflated 77%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/rng_state.pth (deflated 25%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/scaler.pt (deflated 60%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-540/tokenizer.json (deflated 81%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/tokenizer_config.json (deflated 88%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/README.md (deflated 66%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/adapter_config.json (deflated 56%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/chat_template.jinja (deflated 57%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/special_tokens_map.json (deflated 58%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/ (stored 0%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/training_args.bin (deflated 51%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/optimizer.pt (deflated 9%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/tokenizer_config.json (deflated 88%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/README.md (deflated 66%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/adapter_config.json (deflated 56%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/chat_template.jinja (deflated 57%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/scheduler.pt (deflated 55%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/special_tokens_map.json (deflated 58%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/trainer_state.json (deflated 76%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/scaler.pt (deflated 60%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/checkpoint-500/tokenizer.json (deflated 81%)\n  adding: kaggle/working/deepseek-coder-finetuned3b/tokenizer.json (deflated 81%)\n","output_type":"stream"}],"execution_count":12}]}