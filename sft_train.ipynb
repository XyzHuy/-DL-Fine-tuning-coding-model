{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T14:57:21.575158Z",
     "iopub.status.busy": "2025-11-12T14:57:21.574846Z",
     "iopub.status.idle": "2025-11-12T14:58:54.916693Z",
     "shell.execute_reply": "2025-11-12T14:58:54.915930Z",
     "shell.execute_reply.started": "2025-11-12T14:57:21.575136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate peft bitsandbytes datasets trl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T12:51:25.625406Z",
     "iopub.status.busy": "2025-11-12T12:51:25.625114Z",
     "iopub.status.idle": "2025-11-12T12:51:31.863823Z",
     "shell.execute_reply": "2025-11-12T12:51:31.863179Z",
     "shell.execute_reply.started": "2025-11-12T12:51:25.625377Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tìm thấy 313 cặp Problems/Solution khớp số.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution'],\n",
       "    num_rows: 313\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "PROBLEMS_DIR = \"/kaggle/input/fine-tune-dl-btl/problems\"\n",
    "SOLUTIONS_DIR = \"/kaggle/input/fine-tune-dl-btl/solution\"\n",
    "\n",
    "# Lấy danh sách số từ tên file Problem{nums}.md\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'Problem(\\d+)\\.md', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Tạo mapping số → nội dung\n",
    "problem_files = [f for f in os.listdir(PROBLEMS_DIR) if f.endswith('.md')]\n",
    "solution_files = [f for f in os.listdir(SOLUTIONS_DIR) if f.endswith('.py')]\n",
    "\n",
    "# Tạo dict: num → problem content\n",
    "problems_dict = {}\n",
    "for pf in problem_files:\n",
    "    num = extract_number(pf)\n",
    "    if num is not None:\n",
    "        with open(os.path.join(PROBLEMS_DIR, pf), 'r', encoding='utf-8') as f:\n",
    "            problems_dict[num] = f.read().strip()\n",
    "\n",
    "# Tạo dict: num → solution content\n",
    "solutions_dict = {}\n",
    "for sf in solution_files:\n",
    "    match = re.search(r'Solution(\\d+)\\.py', sf)\n",
    "    if match:\n",
    "        num = int(match.group(1))\n",
    "        with open(os.path.join(SOLUTIONS_DIR, sf), 'r', encoding='utf-8') as f:\n",
    "            solutions_dict[num] = f.read().strip()\n",
    "\n",
    "# Ghép cặp theo số\n",
    "paired_data = []\n",
    "common_nums = sorted(set(problems_dict.keys()) & set(solutions_dict.keys()))\n",
    "print(f\"Tìm thấy {len(common_nums)} cặp Problems/Solution khớp số.\")\n",
    "\n",
    "for num in common_nums:\n",
    "    paired_data.append({\n",
    "        \"problem\": problems_dict[num],\n",
    "        \"solution\": solutions_dict[num]\n",
    "    })\n",
    "\n",
    "# Tạo Hugging Face Dataset\n",
    "dataset = Dataset.from_list(paired_data)\n",
    "\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T12:51:36.524287Z",
     "iopub.status.busy": "2025-11-12T12:51:36.523620Z",
     "iopub.status.idle": "2025-11-12T12:53:02.728095Z",
     "shell.execute_reply": "2025-11-12T12:53:02.727313Z",
     "shell.execute_reply.started": "2025-11-12T12:51:36.524260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 12:51:43.873568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762951904.059149      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762951904.102770      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c821bbf89b401a81ab1570f8f15a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defa1f74c72047ca9f64e6580b45a2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152c0946eb014632acc7347c05858f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447fcb8442fa4adbaae6712fcdac5b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4825e779c45c7a84089267b3bd8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c69cb50526340f784638031c7caae78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408bdef201ff4a498d7b3a0e9ed6676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "model_name = \"bigcode/starcoder2-3b\"\n",
    "\n",
    "# load tokenizer + model dùng 4-bit)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# bật gradient checkpointing + LoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# warning không ảnh hưởng âu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T12:53:09.615953Z",
     "iopub.status.busy": "2025-11-12T12:53:09.615539Z",
     "iopub.status.idle": "2025-11-12T12:53:09.670637Z",
     "shell.execute_reply": "2025-11-12T12:53:09.669933Z",
     "shell.execute_reply.started": "2025-11-12T12:53:09.615914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\\nYou can return the answer in any order.\\n \\nExample 1:\\n\\nInput: nums = [2,7,11,15], target = 9\\nOutput: [0,1]\\nExplanation: Because nums[0] + nums[1] == 9, we return [0, 1].\\n\\nExample 2:\\n\\nInput: nums = [3,2,4], target = 6\\nOutput: [1,2]\\n\\nExample 3:\\n\\nInput: nums = [3,3], target = 6\\nOutput: [0,1]\\n\\n \\nConstraints:\\n\\n2 <= nums.length <= 104\\n-109 <= nums[i] <= 109\\n-109 <= target <= 109\\nOnly one valid answer exists.\\n\\n \\nFollow-up: Can you come up with an algorithm that is less than O(n2) time complexity?\\n\\n\\nBoilerplate code:\\n```python\\ndef twoSum(nums, target):\\n    ...\\n```',\n",
       " 'solution': 'def twoSum(nums, target):\\n    # two point\\n    nums_index = [(v, index) for index, v in enumerate(nums)]\\n    nums_index.sort()\\n    begin, end = 0, len(nums) - 1\\n    while begin < end:\\n        curr = nums_index[begin][0] + nums_index[end][0]\\n        if curr == target:\\n            return [nums_index[begin][1], nums_index[end][1]]\\n        elif curr < target:\\n            begin += 1\\n        else:\\n            end -= 1'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T12:53:16.407548Z",
     "iopub.status.busy": "2025-11-12T12:53:16.407256Z",
     "iopub.status.idle": "2025-11-12T12:53:16.553499Z",
     "shell.execute_reply": "2025-11-12T12:53:16.552755Z",
     "shell.execute_reply.started": "2025-11-12T12:53:16.407527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f0347fb4ee4e04b4bb9f22ccd27fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating text column:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 313\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tạo cột \"text\" từ problem + solution\n",
    "def build_text(example):\n",
    "    instruction = (\n",
    "        \"Instruction: Write clean and efficient Python code that correctly solves the problem.\"\n",
    "        \"The solution should be the most optimal approach in terms of time and space complexity.\"\n",
    "        \"Do not include testing or extra text.\\n\"\n",
    "    )\n",
    "    return {\n",
    "        \"text\": f\"### Problem:\\n{example['problem']}\\n\\n{instruction}\\n### Solution:\\n{example['solution']}\\n\\n<|end_of_solution|>\"\n",
    "    }\n",
    "\n",
    "\n",
    "#  map — xóa các cột cũ để chỉ giữ \"text\"\n",
    "dataset_text = dataset.map(\n",
    "    build_text,\n",
    "    remove_columns=dataset.column_names,  # xóa 'problem', 'solution'\n",
    "    desc=\"Creating text column\"\n",
    ")\n",
    "dataset_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T12:53:20.250601Z",
     "iopub.status.busy": "2025-11-12T12:53:20.250308Z",
     "iopub.status.idle": "2025-11-12T13:16:05.709785Z",
     "shell.execute_reply": "2025-11-12T13:16:05.709178Z",
     "shell.execute_reply.started": "2025-11-12T12:53:20.250577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f626779ad94fdba3b9f14194daa1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830976d2cee4fec86264c847f8b841f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11896a1313848d984d34abce9605b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 22:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.907900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.727700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.608300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,   # effective batch=8\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"starcoder2-finetuned\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=dataset_text\n",
    "   \n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:23:22.455940Z",
     "iopub.status.busy": "2025-11-12T13:23:22.455601Z",
     "iopub.status.idle": "2025-11-12T13:23:29.803649Z",
     "shell.execute_reply": "2025-11-12T13:23:29.802813Z",
     "shell.execute_reply.started": "2025-11-12T13:23:22.455911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"/kaggle/working/starcoder2-finetuned\",  # thư mục chứa adapter\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/starcoder2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:24:15.061671Z",
     "iopub.status.busy": "2025-11-12T13:24:15.061355Z",
     "iopub.status.idle": "2025-11-12T13:24:15.065552Z",
     "shell.execute_reply": "2025-11-12T13:24:15.064726Z",
     "shell.execute_reply.started": "2025-11-12T13:24:15.061650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "prompt = r'''\n",
    "Given a list of integers `nums` of size `n`, return the sum of the two largest integers.\n",
    "\n",
    "Example:\n",
    "Input: `nums = [1, 2, 3, 4, 5]`\n",
    "Output: `9`\n",
    "\n",
    "Explanation:\n",
    "The two largest integers are 4 and 5, so the sum is 9.\n",
    "\n",
    "Constraints:\n",
    "- `n < 10000`\n",
    "\n",
    "\n",
    "Boilerplate code:\n",
    "```python\n",
    "def sum_of_two_largest(nums):\n",
    "    ...\n",
    "```\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:24:17.243774Z",
     "iopub.status.busy": "2025-11-12T13:24:17.243491Z",
     "iopub.status.idle": "2025-11-12T13:24:17.247490Z",
     "shell.execute_reply": "2025-11-12T13:24:17.246695Z",
     "shell.execute_reply.started": "2025-11-12T13:24:17.243751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "formatted_prompt = f\"\"\"### Problem:\n",
    "{prompt.strip()}\n",
    "\n",
    "\"Instruction: Write clean and efficient Python code that correctly solves the problem.\"\n",
    "\"The solution should be the most optimal approach in terms of time and space complexity.\"\n",
    "\"Do not include testing or extra text.\\n\"\n",
    "\n",
    "### Solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:24:19.126212Z",
     "iopub.status.busy": "2025-11-12T13:24:19.125577Z",
     "iopub.status.idle": "2025-11-12T13:24:26.150548Z",
     "shell.execute_reply": "2025-11-12T13:24:26.149894Z",
     "shell.execute_reply.started": "2025-11-12T13:24:19.126187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Code ===\n",
      "def sum_of_two_largest(nums):\n",
      "    \"\"\"\n",
      "    :type nums: List[int]\n",
      "    :rtype: int\n",
      "    \"\"\"\n",
      "    largest = second_largest = -sys.maxsize\n",
      "    for num in nums:\n",
      "        if num > largest:\n",
      "            second_largest = largest\n",
      "            largest = num\n",
      "        elif num > second_largest:\n",
      "            second_largest = num\n",
      "    return largest + second_largest\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=False,\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Cấu hình sinh text\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Sinh\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "# Giải mã\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Chỉ lấy phần sau \"### Solution:\"\n",
    "generated_code = full_output.split(\"### Solution:\")[-1].strip()\n",
    "generated_code = generated_code.split(\"\\n\\n<|end_of_solution|>\")[0].strip()\n",
    "\n",
    "print(\"=== Generated Code ===\")\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:18:33.384989Z",
     "iopub.status.busy": "2025-11-12T13:18:33.384676Z",
     "iopub.status.idle": "2025-11-12T13:18:38.609646Z",
     "shell.execute_reply": "2025-11-12T13:18:38.608624Z",
     "shell.execute_reply.started": "2025-11-12T13:18:33.384967Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/starcoder2-finetuned/ (stored 0%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/adapter_model.safetensors"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 8%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/tokenizer_config.json (deflated 90%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/training_args.bin (deflated 51%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/adapter_config.json (deflated 56%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/merges.txt (deflated 51%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/README.md (deflated 44%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/ (stored 0%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/tokenizer_config.json (deflated 90%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/training_args.bin (deflated 51%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/adapter_config.json (deflated 56%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/merges.txt (deflated 51%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/optimizer.pt (deflated 11%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/README.md (deflated 65%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/vocab.json (deflated 57%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/scaler.pt (deflated 60%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/special_tokens_map.json (deflated 72%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/trainer_state.json (deflated 72%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/tokenizer.json (deflated 81%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/vocab.json (deflated 57%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/special_tokens_map.json (deflated 72%)\n",
      "  adding: kaggle/working/starcoder2-finetuned/tokenizer.json (deflated 81%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/starcoder2-finetuned.zip /kaggle/working/starcoder2-finetuned\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8719227,
     "sourceId": 13706352,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
