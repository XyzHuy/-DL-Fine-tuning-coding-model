{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13706352,"sourceType":"datasetVersion","datasetId":8719227}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate peft bitsandbytes datasets trl ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T14:57:21.574846Z","iopub.execute_input":"2025-11-12T14:57:21.575158Z","iopub.status.idle":"2025-11-12T14:58:54.916693Z","shell.execute_reply.started":"2025-11-12T14:57:21.575136Z","shell.execute_reply":"2025-11-12T14:58:54.915930Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\nfrom datasets import Dataset\n\nPROBLEMS_DIR = \"/kaggle/input/fine-tune-dl-btl/problems\"\nSOLUTIONS_DIR = \"/kaggle/input/fine-tune-dl-btl/solution\"\n\n# Lấy danh sách số từ tên file Problem{nums}.md\ndef extract_number(filename):\n    match = re.search(r'Problem(\\d+)\\.md', filename)\n    return int(match.group(1)) if match else None\n\n# Tạo mapping số → nội dung\nproblem_files = [f for f in os.listdir(PROBLEMS_DIR) if f.endswith('.md')]\nsolution_files = [f for f in os.listdir(SOLUTIONS_DIR) if f.endswith('.py')]\n\n# Tạo dict: num → problem content\nproblems_dict = {}\nfor pf in problem_files:\n    num = extract_number(pf)\n    if num is not None:\n        with open(os.path.join(PROBLEMS_DIR, pf), 'r', encoding='utf-8') as f:\n            problems_dict[num] = f.read().strip()\n\n# Tạo dict: num → solution content\nsolutions_dict = {}\nfor sf in solution_files:\n    match = re.search(r'Solution(\\d+)\\.py', sf)\n    if match:\n        num = int(match.group(1))\n        with open(os.path.join(SOLUTIONS_DIR, sf), 'r', encoding='utf-8') as f:\n            solutions_dict[num] = f.read().strip()\n\n# Ghép cặp theo số\npaired_data = []\ncommon_nums = sorted(set(problems_dict.keys()) & set(solutions_dict.keys()))\nprint(f\"Tìm thấy {len(common_nums)} cặp Problems/Solution khớp số.\")\n\nfor num in common_nums:\n    paired_data.append({\n        \"problem\": problems_dict[num],\n        \"solution\": solutions_dict[num]\n    })\n\n# Tạo Hugging Face Dataset\ndataset = Dataset.from_list(paired_data)\n\n\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:51:25.625114Z","iopub.execute_input":"2025-11-12T12:51:25.625406Z","iopub.status.idle":"2025-11-12T12:51:31.863823Z","shell.execute_reply.started":"2025-11-12T12:51:25.625377Z","shell.execute_reply":"2025-11-12T12:51:31.863179Z"}},"outputs":[{"name":"stdout","text":"Tìm thấy 313 cặp Problems/Solution khớp số.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['problem', 'solution'],\n    num_rows: 313\n})"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport torch\n\nmodel_name = \"bigcode/starcoder2-3b\"\n\n# load tokenizer + model dùng 4-bit)\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n)\n\n# bật gradient checkpointing + LoRA\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n)\n\n# warning không ảnh hưởng âu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:51:36.523620Z","iopub.execute_input":"2025-11-12T12:51:36.524287Z","iopub.status.idle":"2025-11-12T12:53:02.728095Z","shell.execute_reply.started":"2025-11-12T12:51:36.524260Z","shell.execute_reply":"2025-11-12T12:53:02.727313Z"}},"outputs":[{"name":"stderr","text":"2025-11-12 12:51:43.873568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762951904.059149      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762951904.102770      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c821bbf89b401a81ab1570f8f15a3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"defa1f74c72047ca9f64e6580b45a2bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152c0946eb014632acc7347c05858f93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447fcb8442fa4adbaae6712fcdac5b18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e4825e779c45c7a84089267b3bd8fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c69cb50526340f784638031c7caae78"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/12.1G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"408bdef201ff4a498d7b3a0e9ed6676b"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:53:09.615539Z","iopub.execute_input":"2025-11-12T12:53:09.615953Z","iopub.status.idle":"2025-11-12T12:53:09.670637Z","shell.execute_reply.started":"2025-11-12T12:53:09.615914Z","shell.execute_reply":"2025-11-12T12:53:09.669933Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'problem': 'Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\\nYou can return the answer in any order.\\n \\nExample 1:\\n\\nInput: nums = [2,7,11,15], target = 9\\nOutput: [0,1]\\nExplanation: Because nums[0] + nums[1] == 9, we return [0, 1].\\n\\nExample 2:\\n\\nInput: nums = [3,2,4], target = 6\\nOutput: [1,2]\\n\\nExample 3:\\n\\nInput: nums = [3,3], target = 6\\nOutput: [0,1]\\n\\n \\nConstraints:\\n\\n2 <= nums.length <= 104\\n-109 <= nums[i] <= 109\\n-109 <= target <= 109\\nOnly one valid answer exists.\\n\\n \\nFollow-up: Can you come up with an algorithm that is less than O(n2) time complexity?\\n\\n\\nBoilerplate code:\\n```python\\ndef twoSum(nums, target):\\n    ...\\n```',\n 'solution': 'def twoSum(nums, target):\\n    # two point\\n    nums_index = [(v, index) for index, v in enumerate(nums)]\\n    nums_index.sort()\\n    begin, end = 0, len(nums) - 1\\n    while begin < end:\\n        curr = nums_index[begin][0] + nums_index[end][0]\\n        if curr == target:\\n            return [nums_index[begin][1], nums_index[end][1]]\\n        elif curr < target:\\n            begin += 1\\n        else:\\n            end -= 1'}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# tạo cột \"text\" từ problem + solution\ndef build_text(example):\n    instruction = (\n        \"Instruction: Write clean and efficient Python code that correctly solves the problem.\"\n        \"The solution should be the most optimal approach in terms of time and space complexity.\"\n        \"Do not include testing or extra text.\\n\"\n    )\n    return {\n        \"text\": f\"### Problem:\\n{example['problem']}\\n\\n{instruction}\\n### Solution:\\n{example['solution']}\\n\\n<|end_of_solution|>\"\n    }\n\n\n#  map — xóa các cột cũ để chỉ giữ \"text\"\ndataset_text = dataset.map(\n    build_text,\n    remove_columns=dataset.column_names,  # xóa 'problem', 'solution'\n    desc=\"Creating text column\"\n)\ndataset_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:53:16.407256Z","iopub.execute_input":"2025-11-12T12:53:16.407548Z","iopub.status.idle":"2025-11-12T12:53:16.553499Z","shell.execute_reply.started":"2025-11-12T12:53:16.407527Z","shell.execute_reply":"2025-11-12T12:53:16.552755Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Creating text column:   0%|          | 0/313 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f0347fb4ee4e04b4bb9f22ccd27fba"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 313\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\n# Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=8,   # effective batch=8\n        num_train_epochs=3,\n        learning_rate=2e-4,\n        fp16=True,\n        logging_steps=10,\n        output_dir=\"starcoder2-finetuned\",\n        optim=\"paged_adamw_8bit\",\n        report_to=\"none\"\n    ),\n    peft_config=peft_config,\n    train_dataset=dataset_text\n   \n)\ntrainer.train()\ntrainer.save_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T12:53:20.250308Z","iopub.execute_input":"2025-11-12T12:53:20.250601Z","iopub.status.idle":"2025-11-12T13:16:05.709785Z","shell.execute_reply.started":"2025-11-12T12:53:20.250577Z","shell.execute_reply":"2025-11-12T13:16:05.709178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f626779ad94fdba3b9f14194daa1da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3830976d2cee4fec86264c847f8b841f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/313 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e11896a1313848d984d34abce9605b8d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 22:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.069000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.595100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.169200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.907900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.758300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.727700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.742900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.678700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.608300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.665000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.588500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.672800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"/kaggle/working/starcoder2-finetuned\",  # thư mục chứa adapter\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/starcoder2-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:23:22.455601Z","iopub.execute_input":"2025-11-12T13:23:22.455940Z","iopub.status.idle":"2025-11-12T13:23:29.803649Z","shell.execute_reply.started":"2025-11-12T13:23:22.455911Z","shell.execute_reply":"2025-11-12T13:23:29.802813Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nprompt = r'''\nGiven a list of integers `nums` of size `n`, return the sum of the two largest integers.\n\nExample:\nInput: `nums = [1, 2, 3, 4, 5]`\nOutput: `9`\n\nExplanation:\nThe two largest integers are 4 and 5, so the sum is 9.\n\nConstraints:\n- `n < 10000`\n\n\nBoilerplate code:\n```python\ndef sum_of_two_largest(nums):\n    ...\n```\n\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:24:15.061355Z","iopub.execute_input":"2025-11-12T13:24:15.061671Z","iopub.status.idle":"2025-11-12T13:24:15.065552Z","shell.execute_reply.started":"2025-11-12T13:24:15.061650Z","shell.execute_reply":"2025-11-12T13:24:15.064726Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"formatted_prompt = f\"\"\"### Problem:\n{prompt.strip()}\n\n\"Instruction: Write clean and efficient Python code that correctly solves the problem.\"\n\"The solution should be the most optimal approach in terms of time and space complexity.\"\n\"Do not include testing or extra text.\\n\"\n\n### Solution:\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:24:17.243491Z","iopub.execute_input":"2025-11-12T13:24:17.243774Z","iopub.status.idle":"2025-11-12T13:24:17.247490Z","shell.execute_reply.started":"2025-11-12T13:24:17.243751Z","shell.execute_reply":"2025-11-12T13:24:17.246695Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Tokenize\ninputs = tokenizer(\n    formatted_prompt,\n    return_tensors=\"pt\",\n    padding=False,\n    truncation=True,\n    max_length=2048\n).to(\"cuda\")\n\n# Cấu hình sinh text\ngeneration_config = GenerationConfig(\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.2,\n    top_p=0.95,\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\n\n# Sinh\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        generation_config=generation_config\n    )\n\n# Giải mã\nfull_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Chỉ lấy phần sau \"### Solution:\"\ngenerated_code = full_output.split(\"### Solution:\")[-1].strip()\ngenerated_code = generated_code.split(\"\\n\\n<|end_of_solution|>\")[0].strip()\n\nprint(\"=== Generated Code ===\")\nprint(generated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:24:19.125577Z","iopub.execute_input":"2025-11-12T13:24:19.126212Z","iopub.status.idle":"2025-11-12T13:24:26.150548Z","shell.execute_reply.started":"2025-11-12T13:24:19.126187Z","shell.execute_reply":"2025-11-12T13:24:26.149894Z"}},"outputs":[{"name":"stderr","text":"`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 0}. If this is not desired, please set these values explicitly.\n","output_type":"stream"},{"name":"stdout","text":"=== Generated Code ===\ndef sum_of_two_largest(nums):\n    \"\"\"\n    :type nums: List[int]\n    :rtype: int\n    \"\"\"\n    largest = second_largest = -sys.maxsize\n    for num in nums:\n        if num > largest:\n            second_largest = largest\n            largest = num\n        elif num > second_largest:\n            second_largest = num\n    return largest + second_largest\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!zip -r /kaggle/working/starcoder2-finetuned.zip /kaggle/working/starcoder2-finetuned\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T13:18:33.384676Z","iopub.execute_input":"2025-11-12T13:18:33.384989Z","iopub.status.idle":"2025-11-12T13:18:38.609646Z","shell.execute_reply.started":"2025-11-12T13:18:33.384967Z","shell.execute_reply":"2025-11-12T13:18:38.608624Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/starcoder2-finetuned/ (stored 0%)\n  adding: kaggle/working/starcoder2-finetuned/adapter_model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 8%)\n  adding: kaggle/working/starcoder2-finetuned/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/starcoder2-finetuned/training_args.bin (deflated 51%)\n  adding: kaggle/working/starcoder2-finetuned/adapter_config.json (deflated 56%)\n  adding: kaggle/working/starcoder2-finetuned/merges.txt (deflated 51%)\n  adding: kaggle/working/starcoder2-finetuned/README.md (deflated 44%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/ (stored 0%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/tokenizer_config.json (deflated 90%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/training_args.bin (deflated 51%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/adapter_config.json (deflated 56%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/scheduler.pt (deflated 56%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/merges.txt (deflated 51%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/optimizer.pt (deflated 11%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/README.md (deflated 65%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/vocab.json (deflated 57%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/scaler.pt (deflated 60%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/special_tokens_map.json (deflated 72%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/trainer_state.json (deflated 72%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/rng_state.pth (deflated 25%)\n  adding: kaggle/working/starcoder2-finetuned/checkpoint-120/tokenizer.json (deflated 81%)\n  adding: kaggle/working/starcoder2-finetuned/vocab.json (deflated 57%)\n  adding: kaggle/working/starcoder2-finetuned/special_tokens_map.json (deflated 72%)\n  adding: kaggle/working/starcoder2-finetuned/tokenizer.json (deflated 81%)\n","output_type":"stream"}],"execution_count":7}]}